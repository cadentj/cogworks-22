{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7017cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mygrad as mg\n",
    "from mygrad import Tensor\n",
    "\n",
    "from noggin import create_plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "882e96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "class Database:\n",
    "    def get_IDF(word):\n",
    "        return np.log10( (len(coco_data[\"annotations\"])) / len(img2captions[word]))\n",
    "    \n",
    "    def get_gloveE(caption): \n",
    "        return glove[caption]\n",
    "    \n",
    "    def get_word_vectors(line):\n",
    "        strip_punc(line)\n",
    "        line = line.lower()\n",
    "        arr = line.split()\n",
    "        verctor_sum = 0\n",
    "        for word in arr:\n",
    "            verctor_sum += (get_IDF(word) * get_glovE(word))\n",
    "        return verctor_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88979d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary 3:\n",
    "#caption-ID -> caption (e.g. 24 -> \"two dogs on the grass\")\n",
    "\n",
    "capID2cap = {}\n",
    "\n",
    "for caption in coco_data[\"annotations\"]:\n",
    "    capID2cap[caption[\"id\"]] = caption[\"caption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary 2:\n",
    "#caption-ID -> image-ID\n",
    "\n",
    "cap2img = {}\n",
    "\n",
    "for caption in coco_data[\"annotations\"]:\n",
    "    cap2img[caption[\"id\"]] = caption[\"image_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary 1:\n",
    "#image-ID -> [cap-ID-1, cap-ID-2, ...]\n",
    "\n",
    "img2captions = {}\n",
    "\n",
    "for cap in coco_data[\"annotations\"]:\n",
    "    if cap[\"image_id\"] not in img2captions:\n",
    "        img2captions[cap[\"image_id\"]] = []\n",
    "    img2captions[cap[\"image_id\"]].append(cap[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cf20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasplit(X):\n",
    "  X_train, X_test = train_test_split(X, train_size=0.8, random_state=50)\n",
    "  return (X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f17e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#done!\n",
    "def generateTriples(imglist, img2captions):\n",
    "  ret = []\n",
    "  for img in imglist:\n",
    "    for caption in img2captions[img]:\n",
    "      fake = np.random.choice(imglist)\n",
    "      while (fake == img):\n",
    "        fake = np.random.choice(imglist)\n",
    "      ret.append((caption, img, fake))\n",
    "  return ret\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6317a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img2captions = {1: [100, 200, 300], 2: [1,2,3], 3:[4,5], 4:[6]}\n",
    "imglist = [1,2,3,4]\n",
    "train, test = datasplit(imglist)\n",
    "hehe = generateTriples(train, img2captions)\n",
    "print(hehe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.layers.dense import dense\n",
    "from mygrad.nnet.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ceb70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, D_input=512, D_glove=200):\n",
    "        self.dense1 = dense(D_input, D_glove, weight_initializer=glorot_normal) \n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.dense1(x)\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self.dense1.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf6ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.optimizers.sgd import SGD\n",
    "from mygrad.nnet.losses import margin_ranking_loss\n",
    "\n",
    "optim = SGD(model.parameters, learning_rate=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fdb1a20",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4090296465.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    |caption2, confusion_image, truth_image|\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# (1,200)       (1,512)          (1,512)\n",
    "\n",
    "# [caption1, confusion_image, truth_image]\n",
    "# |caption2, confusion_image, truth_image|\n",
    "# [caption3, confusion_image, truth_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20367d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction_truth, prediction_confusion, truth_caption):\n",
    "    truth_count = np.count_nonzero(np.dot(prediction_truth, truth_caption) > np.dot(prediction_confusion, truth_caption))\n",
    "    return truth_count / len(truth_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "\n",
    "model = Model(512, 200)\n",
    "\n",
    "for epoch_cnt in range(num_epochs):\n",
    "    idxs = np.arange(len(train)) \n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    for batch_cnt in range(len(train) // batch_size):\n",
    "        batch_indices = idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size]\n",
    "        \n",
    "        # captions\n",
    "        truth_caption_ID = train[batch_indices][0]\n",
    "        truth_caption = truth_caption[truth_caption_ID]\n",
    "        #image and confusion\n",
    "        truth_image = train[batch_indices][1]\n",
    "        confusion = train[batch_indices][2]\n",
    "        \n",
    "        prediction_truth = model(truth_image)  \n",
    "        prediction_confusion = model(confusion)\n",
    "        \n",
    "        sim_truth = np.dot(prediction_truth, truth_caption)\n",
    "        sim_confusion =  np.dot(prediction_confusion, truth_caption)\n",
    "        \n",
    "        # add einsum \n",
    "        loss = margin_ranking_loss(sim_truth, sim_confusion, truth_caption, margin=0.25) \n",
    "\n",
    "        loss.backward() \n",
    "\n",
    "        optim.step() \n",
    "\n",
    "        acc = accuracy(prediction_truth, prediction_confusion, truth_caption)\n",
    "        \n",
    "        plotter.set_train_batch(\n",
    "            {\"loss\": loss.item(), \"accuracy\": acc}, batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "    for batch_cnt in range(0, len(x_test) // batch_size):\n",
    "        idxs = np.arange(len(x_test))\n",
    "        batch_indices = idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size] \n",
    "        batch = x_test[batch_indices]\n",
    "\n",
    "        with mg.no_autodiff:\n",
    "            # captions\n",
    "            truth_caption_ID = test[batch_indices][0]\n",
    "            truth_caption = truth_caption[truth_caption_ID]\n",
    "            #image and confusion\n",
    "            truth_image = test[batch_indices][1]\n",
    "            confusion = test[batch_indices][2]\n",
    "\n",
    "            prediction_truth = model(truth_image)  \n",
    "            prediction_confusion = model(confusion)\n",
    "            \n",
    "            acc = accuracy(prediction_truth, prediction_confusion, truth_caption)\n",
    "\n",
    "        plotter.set_test_batch({\"accuracy\": acc}, batch_size=batch_size)\n",
    "\n",
    "    plotter.set_train_epoch()\n",
    "    plotter.set_test_epoch()\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eba367",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array(IDs)\n",
    "# only fill w validation set images to know that the quality of our results isn't overfitting on our data\n",
    "imageVectors = np.array(vectors)\n",
    "\n",
    "def query(captionVector):\n",
    "    dot = np.dot(imageVectors, captionVector)\n",
    "    return ids[np.argpartition(dot, -4)[-4:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2befe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "def download_image(img_url: str) -> Image:\n",
    "    response = requests.get(img_url)\n",
    "    return Image.open(io.BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "  \n",
    "# create figure\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "  \n",
    "# setting values to rows and column variables\n",
    "rows = 2\n",
    "columns = 2\n",
    "  \n",
    "images = query(caption_vector)\n",
    "# reading images\n",
    "Image1 = coca_data[images[0]].url\n",
    "Image2 = coca_data[images[1]].url\n",
    "Image3 = coca_data[images[2]].url\n",
    "Image4 = coca_data[images[3]].url\n",
    "  \n",
    "# Adds a subplot at the 1st position\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image1)\n",
    "plt.axis('off')\n",
    "plt.title(\"First\")\n",
    "  \n",
    "# Adds a subplot at the 2nd position\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image2)\n",
    "plt.axis('off')\n",
    "plt.title(\"Second\")\n",
    "  \n",
    "# Adds a subplot at the 3rd position\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image3)\n",
    "plt.axis('off')\n",
    "plt.title(\"Third\")\n",
    "  \n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 4)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image4)\n",
    "plt.axis('off')\n",
    "plt.title(\"Fourth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
