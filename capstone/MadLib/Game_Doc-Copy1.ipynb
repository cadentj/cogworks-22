{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d14ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1690611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(pairs):\n",
    "    return tuple(zip(*pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc44db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(counter):\n",
    "    total = sum(counter.values())\n",
    "    return [(char, cnt/total) for char, cnt in counter.most_common()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de7a8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "def train_lm(text, n):\n",
    "    raw_lm = defaultdict(Counter) # history -> {char -> count}\n",
    "    history = \"~\" * (n - 1)  # length n - 1 history\n",
    "    \n",
    "    # count number of times characters appear following different histories\n",
    "    #\n",
    "    # for char in text ...\n",
    "    #    1. Increment language model's count, given current history and character\n",
    "    #    2. Update history\n",
    "\n",
    "    for char in text:\n",
    "        raw_lm[history][char] += 1\n",
    "        # slide history window to the right by one character\n",
    "        history = history[1:] + char\n",
    "\n",
    "    \n",
    "    # create the finalized language model â€“ a dictionary with: history -> [(char, freq), ...]\n",
    "    lm = {history : normalize(counter) for history, counter in raw_lm.items()} \n",
    "    \n",
    "    return lm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a58ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_letter(lm, history):\n",
    "    if not history in lm:\n",
    "        return \"~\"\n",
    "    letters, probs = unzip(lm[history])\n",
    "    i = np.random.choice(letters, p=probs)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e8d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, n, nletters=100):\n",
    "    # <COGINST>\n",
    "    history = \"~\" * (n - 1)\n",
    "    text = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history)\n",
    "        text.append(c)\n",
    "        history = history[1:] + c\n",
    "    return \"\".join(text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf54030",
   "metadata": {},
   "source": [
    "## 2. Generating \"The Percy Jackson Series\"\n",
    "\n",
    "The next cell loads in pjallbooks.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13b571c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "path_to_pj = \"/Users/manyadua/Downloads/ryan-sus/capstone/MadLib/pjallbooks.txt\"\n",
    "\n",
    "#get_data_path(\"pjolympians.txt\")\n",
    "\n",
    "with open(path_to_pj, \"rb\") as f:\n",
    "    pj = f.read().decode()  \n",
    "    pj = pj.lower()  \n",
    "    pj.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbda5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text =\" It was the first day of the rest of her life. This wasn't the day she was actually born, but she knew that nothing would be the same from this day forward. Although this was a bit scary to her, it was also extremely freeing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc94e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/manyadua/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7dadbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a38e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('first', 'JJ'), ('day', 'NN'), ('of', 'IN'), ('the', 'DT'), ('rest', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('life', 'NN'), ('.', '.'), ('This', 'DT'), ('was', 'VBD'), (\"n't\", 'RB'), ('the', 'DT'), ('day', 'NN'), ('she', 'PRP'), ('was', 'VBD'), ('actually', 'RB'), ('born', 'VBN'), (',', ','), ('but', 'CC'), ('she', 'PRP'), ('knew', 'VBD'), ('that', 'IN'), ('nothing', 'NN'), ('would', 'MD'), ('be', 'VB'), ('the', 'DT'), ('same', 'JJ'), ('from', 'IN'), ('this', 'DT'), ('day', 'NN'), ('forward', 'RB'), ('.', '.'), ('Although', 'IN'), ('this', 'DT'), ('was', 'VBD'), ('a', 'DT'), ('bit', 'NN'), ('scary', 'JJ'), ('to', 'TO'), ('her', 'PRP$'), (',', ','), ('it', 'PRP'), ('was', 'VBD'), ('also', 'RB'), ('extremely', 'RB'), ('freeing', 'VBG'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tok_text = word_tokenize(new_text)\n",
    "gram = nltk.pos_tag(tok_text)\n",
    "print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f098407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter noun 1: apple\n",
      "Please enter noun 2: orange\n",
      "Please enter noun 3: banana\n",
      "Please enter noun 4: grapes\n",
      "Please enter noun 5: peach\n"
     ]
    }
   ],
   "source": [
    "list_words = []\n",
    "for i in range(1,6):\n",
    "    list_words.append(input(\"Please enter noun \" + str(i) + \": \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04ad507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## [mnay, jakjdfl, ajdfkj]\n",
    "\n",
    "# counter = 0 \n",
    "# for i in range(len(gram)):\n",
    "#     if gram[i][1] == \"NN\" and counter < 5: \n",
    "#         gram[i][0] = list_words[counter]\n",
    "#         counter += 1\n",
    "# print(gram)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45b21dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'first', 'day', 'of', 'the', 'rest', 'of', 'her', 'life', '.', 'This', 'was', \"n't\", 'the', 'day', 'she', 'was', 'actually', 'born', ',', 'but', 'she', 'knew', 'that', 'nothing', 'would', 'be', 'the', 'same', 'from', 'this', 'day', 'forward', '.', 'Although', 'this', 'was', 'a', 'bit', 'scary', 'to', 'her', ',', 'it', 'was', 'also', 'extremely', 'freeing', '.']\n",
      "['PRP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', '.', 'DT', 'VBD', 'RB', 'DT', 'NN', 'PRP', 'VBD', 'RB', 'VBN', ',', 'CC', 'PRP', 'VBD', 'IN', 'NN', 'MD', 'VB', 'DT', 'JJ', 'IN', 'DT', 'NN', 'RB', '.', 'IN', 'DT', 'VBD', 'DT', 'NN', 'JJ', 'TO', 'PRP$', ',', 'PRP', 'VBD', 'RB', 'RB', 'VBG', '.']\n"
     ]
    }
   ],
   "source": [
    "## tokenize two arrays \n",
    "## it \n",
    "## NN \n",
    "\n",
    "words = [] \n",
    "parts_of_speech = []\n",
    "\n",
    "for i in gram: \n",
    "    words.append(i[0])\n",
    "    parts_of_speech.append(i[1])\n",
    "\n",
    "print(words)\n",
    "print(parts_of_speech)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1eb7f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode 1 - just nouns \n",
    "# adjective, nouns, verbs \n",
    "# crazy mode - cover more of the text with adjs, nouns, verbs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d11fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ff198d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(parts_of_speech)\n",
    "\n",
    "adj_count = c[\"NN\"]\n",
    "verb_count = c[\"VB\"] + c[\"VBR\"]\n",
    "noun_count = c[\"JJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ff0ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_words(code, w_max, wds):\n",
    "    counter = 0 \n",
    "    for j in range(len(parts_of_speech)): \n",
    "        if parts_of_speech[j] == code and counter < w_max :\n",
    "            wds[j] = list_words[counter]\n",
    "            counter += 1\n",
    "    return wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69ba72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_mode(): #returns an array of tuples with words and speech matchings \n",
    "    \n",
    "    lmpj1 = train_lm(pj, 15)\n",
    "    new_text = generate_text(lmpj1, 15,1000)\n",
    "    new_text = new_text.split()\n",
    "    \n",
    "    tok_text = word_tokenize(new_text)\n",
    "    gram = nltk.pos_tag(tok_text)\n",
    "    \n",
    "    return gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2a2221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(adj_c, verb_c, noun_c,exp, words, mode = None): \n",
    "    final_text = \"\" #stores the updated text \n",
    "    noun_max = (exp/100) * noun_c \n",
    "    adj_max = (exp/100) * adj_c\n",
    "    verb_max = (exp/100) * verb_c\n",
    "    # it is \n",
    "    if mode == 1: #only use verb_count \n",
    "        words = switch_words(\"NN\", noun_max, words)\n",
    "        final_text = ' '.join(words)\n",
    "    \n",
    "    elif mode == 2: \n",
    "        #nouns are already updated, update verbs + adjs\n",
    "        words = switch_words(\"NN\", noun_max, words)\n",
    "        words = switch_words(\"VB\", noun_max, words)\n",
    "        words = switch_words(\"JJ\", noun_max, words)\n",
    "\n",
    "        final_text = ' '.join(words)\n",
    "        \n",
    "    elif mode == 3: #run on our N-gram model \n",
    "        \n",
    "        mode_list = ngram_model()\n",
    "        for i in mode_list: \n",
    "            words.append(i[0])\n",
    "            \n",
    "        words = switch_words(\"NN\", noun_max, words)\n",
    "        words = switch_words(\"VB\", noun_max, words)\n",
    "        words = switch_words(\"JJ\", noun_max, words)\n",
    "\n",
    "        final_text = ' '.join(words) \n",
    "        \n",
    "    return final_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c51e64b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It was the first apple of the rest of her life . This was n't the day she was actually born , but she knew that nothing would be the same from this day forward . Although this was a bit scary to her , it was also extremely freeing .\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(adj_count , verb_count, noun_count , 25, words, mode = 1) \n",
    "#exposure is the percentage amount you want the words to cover in your word list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a27853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #counter = 0 \n",
    "# for value in gram: # (word, part of speech)\n",
    "#     if value[1] == \"NN\" and counter < 5: \n",
    "#         value[0] = list_words[counter]\n",
    "#         counter += 1\n",
    "        \n",
    "# print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6266c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Convert(tup, di):\n",
    "#     di = dict(tup)\n",
    "#     return di\n",
    "# # Driver Code    \n",
    "# gram_dict = {}\n",
    "# Convert(gram, gram_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa356278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# d = defaultdict(list)\n",
    "# for key, value in gram:\n",
    "#     d[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cac1475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'It': ['PRP'], 'was': ['VBD', 'VBD', 'VBD', 'VBD', 'VBD'], 'the': ['DT', 'DT', 'DT', 'DT'], 'first': ['JJ'], 'day': ['NN', 'NN', 'NN'], 'of': ['IN', 'IN'], 'rest': ['NN'], 'her': ['PRP$', 'PRP$'], 'life': ['NN'], '.': ['.', '.', '.'], 'This': ['DT'], \"n't\": ['RB'], 'she': ['PRP', 'PRP'], 'actually': ['RB'], 'born': ['VBN'], ',': [',', ','], 'but': ['CC'], 'knew': ['VBD'], 'that': ['IN'], 'nothing': ['NN'], 'would': ['MD'], 'be': ['VB'], 'same': ['JJ'], 'from': ['IN'], 'this': ['DT', 'DT'], 'forward': ['RB'], 'Although': ['IN'], 'a': ['DT'], 'bit': ['NN'], 'scary': ['JJ'], 'to': ['TO'], 'it': ['PRP'], 'also': ['RB'], 'extremely': ['RB'], 'freeing': ['VBG']})\n"
     ]
    }
   ],
   "source": [
    "# print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a10cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter noun 1: \n",
      "Please enter noun 2: \n"
     ]
    }
   ],
   "source": [
    "# list_words = []\n",
    "# for i in range(1,6):\n",
    "#     list_words.append(input(\"Please enter noun \" + str(i) + \": \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c5167c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grapes\n",
      "orange\n",
      "peach\n",
      "banana\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(list_words)):\n",
    "#     print(list_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf481d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0d/dps258cx4dj0xbr633s_gv2c0000gn/T/ipykernel_20524/1342738188.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# VBD: the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#NN : apple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mg_swap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# swap the values and the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"i_of_nouns = []\n",
    "\n",
    "for key, value in list(g.items()):\n",
    "    if value == 'NN':\n",
    "            del g[key]\n",
    "   \n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2885a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "196e7c86",
   "metadata": {},
   "source": [
    "## Working on making the actual game\n",
    "\n",
    "1. remove random nouns using spacy\n",
    "2. find the pos of the random words\n",
    "3. either leave word blank for user to fill in\n",
    "4. or have computer generate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9158532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f2ce74",
   "metadata": {},
   "source": [
    "## 3. Generating \"Dr. Suess' Work\"\n",
    "\n",
    "The next cell loads in drsuess.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "904a58b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 8, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_5/5tppy8rs7bv1n7qz5flpdysh0000gn/T/ipykernel_96671/2320990424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath_to_seuss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/drseuss.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mseuss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_seuss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#with open(path_to_suess, \"rb\") as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 8, saw 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_to_seuss = \"/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/drseuss.txt\"\n",
    "\n",
    "seuss = pd.read_csv(path_to_seuss, encoding= 'unicode_escape')\n",
    "\n",
    "#with open(path_to_suess, \"rb\") as f:\n",
    "seuss = seuss.read().decode()  \n",
    "seuss = seuss.lower()\n",
    "seuss.split()\n",
    "    \n",
    "\n",
    "\n",
    "print(str(len(suess)) + \" character(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41575302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
