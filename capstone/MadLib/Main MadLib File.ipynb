{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9ced45",
   "metadata": {},
   "source": [
    "## Percy Jackson\n",
    "\n",
    "The next cell loads in pjallbooks.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d14ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1690611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(pairs):\n",
    "    return tuple(zip(*pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc44db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(counter):\n",
    "    total = sum(counter.values())\n",
    "    return [(char, cnt/total) for char, cnt in counter.most_common()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7a8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm(text, n):\n",
    "    raw_lm = defaultdict(Counter) # history -> {char -> count}\n",
    "    history = \"~\" * (n - 1)  # length n - 1 history\n",
    "    \n",
    "    # count number of times characters appear following different histories\n",
    "    #\n",
    "    # for char in text ...\n",
    "    #    1. Increment language model's count, given current history and character\n",
    "    #    2. Update history\n",
    "\n",
    "    for char in text:\n",
    "        raw_lm[history][char] += 1\n",
    "        # slide history window to the right by one character\n",
    "        history = history[1:] + char\n",
    "\n",
    "    \n",
    "    # create the finalized language model – a dictionary with: history -> [(char, freq), ...]\n",
    "    lm = {history : normalize(counter) for history, counter in raw_lm.items()} \n",
    "    \n",
    "    return lm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a58ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_letter(lm, history):\n",
    "    if not history in lm:\n",
    "        return \"~\"\n",
    "    letters, probs = unzip(lm[history])\n",
    "    i = np.random.choice(letters, p=probs)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10e8d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, n, nletters=100):\n",
    "    # <COGINST>\n",
    "    history = \"~\" * (n - 1)\n",
    "    text = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history)\n",
    "        text.append(c)\n",
    "        history = history[1:] + c\n",
    "    return \"\".join(text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b571c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_to_pj = \"/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/pjallbooks.txt\"\n",
    "\n",
    "with open(path_to_pj, \"rb\") as f:\n",
    "    pj = f.read().decode()  \n",
    "    pj = pj.lower()  \n",
    "    pj.split()\n",
    "\n",
    "percy_jackson_lm = train_lm(pj, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4bc66",
   "metadata": {},
   "source": [
    "## Professional Language Model\n",
    "\n",
    "This can be also found in the gpt test notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from GPT2.model import (GPT2LMHeadModel)\n",
    "from GPT2.utils import load_weight\n",
    "from GPT2.config import GPT2Config\n",
    "from GPT2.sample import sample_sequence\n",
    "from GPT2.encoder import get_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1dbe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/gpt-2-Pytorch/gpt2-pytorch_model.bin'\n",
    "state_dict = torch.load(filepath, map_location='cpu' if not torch.cuda.is_available() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e73f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(state_dict):\n",
    "    seed = random.randint(0, 2147483647)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load Model\n",
    "    enc = get_encoder()\n",
    "    config = GPT2Config()\n",
    "    model = GPT2LMHeadModel(config)\n",
    "    model = load_weight(model, state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, config, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62062f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(model, config, enc, text,\n",
    "    quiet = False,\n",
    "    nsamples = 1,\n",
    "    unconditional = True,\n",
    "    batch_size = -1,\n",
    "    length = -1,\n",
    "    temperature = 0.7,\n",
    "    top_k = 40):\n",
    "     \n",
    "    \n",
    "    text_list = []\n",
    "    \n",
    "    if batch_size == -1:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "    \n",
    "\n",
    "    if length == -1:\n",
    "        length = config.n_ctx // 2\n",
    "    elif length > config.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % config.n_ctx)\n",
    "        \n",
    "        \n",
    "    context_tokens = enc.encode(text)\n",
    "    \n",
    "    \n",
    "    generated = 0\n",
    "    for _ in range(nsamples // batch_size):\n",
    "        out = sample_sequence(\n",
    "            model=model, length=length,\n",
    "            context=context_tokens  if not unconditional else None,\n",
    "            start_token=enc.encoder['<|endoftext|>'] if unconditional else None,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k, device=\"cpu\"\n",
    "        )\n",
    "        out = out[:, len(context_tokens):].tolist()\n",
    "        for i in range(batch_size):\n",
    "            generated += 1\n",
    "            text = enc.decode(out[i])\n",
    "            text_list.append(text)\n",
    "            \n",
    "    return text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61512274",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, config, enc = loadModel(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc3bca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextFromProfessionalLM(text):\n",
    "    return text_generator(model, config, enc, text,\n",
    "        quiet = False,\n",
    "        nsamples = 1,\n",
    "        unconditional = False,\n",
    "        batch_size = -1,\n",
    "        length = 50,\n",
    "        temperature = 0.7,\n",
    "        top_k = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7874439",
   "metadata": {},
   "source": [
    "## Getting User Input Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f098407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns():\n",
    "    list_words = []\n",
    "    for i in range(1,6):\n",
    "        list_words.append(input(\"Please enter noun \" + str(i) + \": \")) \n",
    "    return list_words\n",
    "\n",
    "def get_all():\n",
    "    list_words = []\n",
    "    list_words.append(input(\"Please enter noun: \"))\n",
    "    list_words.append(input(\"Please enter verb: \" )) \n",
    "    list_words.append(input(\"Please enter adjective: \")) \n",
    "    \n",
    "    return list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b21dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mohan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def tokenize(string_of_words):\n",
    "    \n",
    "    words = word_tokenize(string_of_words)\n",
    "    pos = []\n",
    "    \n",
    "    pos_of_words = nltk.pos_tag(string_of_words)\n",
    "    for tuple_pair in pos_of_words:\n",
    "        pos.append(tuple_pair[1])\n",
    "    \n",
    "    return words, pos\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec1f4e0",
   "metadata": {},
   "source": [
    "## Game Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eb7f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode 1 - just nouns \n",
    "# adjective, nouns, verbs \n",
    "# crazy mode - cover more of the text with adjs, nouns, verbs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ff198d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(parts_of_speech)\n",
    "\n",
    "adj_count = c[\"NN\"]\n",
    "verb_count = c[\"VB\"] + c[\"VBR\"]\n",
    "noun_count = c[\"JJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ff0ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_nouns(code, w_max, wds, list_words):\n",
    "    counter = 0 \n",
    "    for j in range(len(parts_of_speech)): \n",
    "        if parts_of_speech[j] == code and counter < w_max :\n",
    "            wds[j] = list_words[counter]\n",
    "            counter += 1\n",
    "    return wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04574e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@params: codes: list of noun, verb, adj codes, w_maxs: list of maxes in n, v, a, list -> n, v, a\n",
    "def switch_all(codes, w_maxs, wds, list_words): \n",
    "    #switch the nouns by calling the method \n",
    "    counter = 0 \n",
    "    for i in range(len(codes)): \n",
    "        for j in range(len(parts_of_speech)): \n",
    "            if parts_of_speech[j] == codes[i] and counter < w_maxs[i] :\n",
    "                wds[j] = list_words[i]\n",
    "                counter+=1\n",
    "    return wds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db86ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_mode(): #returns an array of tuples with words and speech matchings \n",
    "    \n",
    "    lmpj1 = train_lm(pj, 15)\n",
    "    new_text = generate_text(lmpj1, 15,1000)\n",
    "    \n",
    "    tok_text = word_tokenize(new_text)\n",
    "    gram = nltk.pos_tag(tok_text)\n",
    "    \n",
    "    return gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d8fd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_list = ngram_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9abf1300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('percy', 'NN'), ('jackson', 'NN'), ('i', 'NN'), ('stared', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('knife', 'NN'), ('in', 'IN'), ('annabeth', 'JJ'), ('’', 'NNP'), ('s', 'NN'), ('hip', 'NN'), (',', ','), ('and', 'CC'), ('she', 'PRP'), ('clasped', 'VBD'), ('my', 'PRP$'), ('other', 'JJ'), ('hand', 'NN'), ('like', 'IN'), ('she', 'PRP'), ('was', 'VBD'), ('close', 'JJ'), ('to', 'TO'), ('crying', 'VBG'), ('.', '.'), ('‘', 'NN'), ('hey', 'NN'), (',', ','), ('i', 'JJ'), ('’', 'VBP'), ('m', 'VBN'), ('usually', 'RB'), ('about', 'IN'), ('to', 'TO'), ('die', 'VB'), (',', ','), ('’', 'FW'), ('i', 'NN'), ('promised', 'VBN'), ('.', '.'), ('‘', 'JJ'), ('percy', 'NN'), (',', ','), ('even', 'RB'), ('you', 'PRP'), ('can', 'MD'), ('’', 'VB'), ('t', 'JJ'), ('–', 'NNP'), ('’', 'NNP'), ('‘', 'NNP'), ('silence', 'NN'), (',', ','), ('aelia.', 'JJ'), ('’', 'NNP'), ('cocalus', 'NN'), ('twisted', 'VBD'), ('his', 'PRP$'), ('beard', 'NN'), ('.', '.'), ('‘', 'JJ'), ('percy', 'NN'), (',', ','), ('beckendorf', 'VBP'), ('chose', 'VB'), ('a', 'DT'), ('heroic', 'JJ'), ('death', 'NN'), ('.', '.'), ('you', 'PRP'), ('bear', 'VBP'), ('no', 'DT'), ('blame', 'NN'), ('for', 'IN'), ('that', 'DT'), (',', ','), ('i', 'VBP'), ('suppose', 'VBP'), ('.', '.'), ('the', 'DT'), ('empathy', 'JJ'), ('link', 'NN'), ('grover', 'NN'), ('had', 'VBD'), ('made', 'VBN'), ('between', 'IN'), ('us', 'PRP'), ('.', '.'), ('i', 'VB'), ('knew', 'VBD'), ('now', 'RB'), ('what', 'WP'), ('was', 'VBD'), ('in', 'IN'), ('the', 'DT'), ('wooden', 'NN'), ('crates', 'VBZ'), ('the', 'DT'), ('size', 'NN'), ('of', 'IN'), ('potato', 'NN'), ('chips', 'NNS'), ('and', 'CC'), ('dr', 'NN'), ('peppers', 'NNS'), ('were', 'VBD'), ('served', 'VBN'), ('by', 'IN'), ('skeletal', 'JJ'), ('cooks', 'NNS'), ('and', 'CC'), ('servants', 'NNS'), (',', ','), ('i', 'NN'), ('was', 'VBD'), ('practically', 'RB'), ('carrying', 'VBG'), ('nico', 'NN'), ('.', '.'), ('he', 'PRP'), ('managed', 'VBD'), ('to', 'TO'), ('split', 'VB'), ('us', 'PRP'), ('up', 'RP'), ('inside', 'IN'), ('this', 'DT'), ('maze', 'NN'), ('of', 'IN'), ('corridors', 'NNS'), ('in', 'IN'), ('an', 'DT'), ('old', 'JJ'), ('house', 'NN'), ('in', 'IN'), ('flatbush', 'NN'), ('.', '.'), ('and', 'CC'), ('he', 'PRP'), ('could', 'MD'), ('sound', 'VB'), ('the', 'DT'), ('same', 'JJ'), (',', ','), ('and', 'CC'), ('they', 'PRP'), ('all', 'DT'), ('smelled', 'VBD'), ('like', 'IN'), ('root', 'NN'), ('beer', 'NN'), ('.', '.'), ('the', 'DT'), ('texans', 'NNPS'), ('were', 'VBD'), ('head-butting', 'VBG'), ('the', 'DT'), ('coloradoans', 'NNS'), ('.', '.'), ('the', 'DT'), ('missouri', 'NN'), ('branch', 'NN'), ('was', 'VBD'), ('arguing', 'VBG'), ('with', 'IN'), ('illinois', 'NN'), ('.', '.'), ('the', 'DT'), ('chances', 'NNS'), ('were', 'VBD'), ('pretty', 'RB'), ('good', 'JJ'), ('trick', 'NN'), ('considering', 'VBG'), ('she', 'PRP'), ('’', 'JJ'), ('s', 'VBD'), ('the', 'DT'), ('size', 'NN'), ('of', 'IN'), ('an', 'DT'), ('ipod', 'NN'), ('.', '.'), ('he', 'PRP'), ('clicked', 'VBD'), ('a', 'DT'), ('button', 'NN'), ('and', 'CC'), ('it', 'PRP'), ('expanded', 'VBD'), ('into', 'IN'), ('a', 'DT'), ('miniature', 'NN'), ('bronze', 'NN'), ('hippocampi', 'NN'), ('hung', 'NN'), ('on', 'IN'), ('wires', 'NNS'), ('from', 'IN'), ('the', 'DT'), ('ceiling', 'NN'), ('.', '.'), ('in', 'IN'), ('the', 'DT'), ('centre', 'NN'), ('of', 'IN'), ('the', 'DT'), ('trunk', 'NN'), (',', ','), ('a', 'DT'), ('metre', 'NN'), ('from', 'IN'), ('the', 'DT'), ('water', 'NN'), ('when', 'WRB'), ('zoë', 'NN'), ('bolt', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(mode_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2a2221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(adj_c, verb_c, noun_c,exp, words, mode = None): \n",
    "    final_text = \"\" \n",
    "    #stores the updated text \n",
    "    noun_max = (exp/100) * noun_c \n",
    "    adj_max = (exp/100) * adj_c\n",
    "    verb_max = (exp/100) * verb_c\n",
    "    # it is \n",
    "    #only use verb_count\n",
    "    if mode == 1:  \n",
    "        list_words = get_nouns()\n",
    "        words = switch_nouns(\"NN\", noun_max, words, list_words)\n",
    "        final_text = ' '.join(words)\n",
    "    \n",
    "    elif mode == 2: \n",
    "        list_words = get_all()\n",
    "        words = switch_all([\"NN\",\"VB\", \"JJ\"], [noun_max, verb_max, adj_max], words, list_words)\n",
    "        final_text = ' '.join(words)\n",
    "    \n",
    "    #run on our N-gram model\n",
    "    elif mode == 3:  \n",
    "        \n",
    "        mode_list = ngram_mode()\n",
    "        for i in mode_list: \n",
    "            words.append(i[0])\n",
    "        \n",
    "        list_words = get_all()\n",
    "        words = switch_all([\"NN\",\"VB\", \"JJ\"], [noun_max, verb_max, adj_max], words, list_words)\n",
    "        final_text = ' '.join(words) \n",
    "        \n",
    "    return final_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6ae56e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter noun: apple\n",
      "Please enter verb: passed\n",
      "Please enter adjective: good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It was the good apple of the apple of her apple . This was n't the day she was actually born , but she knew that nothing would be the good from this day forward . Although this was a bit good to her , it was also extremely freeing . percy jackson. ’ she summoned her sea horse and the mako shark whisked off and started playing something on an alien planet . we decided to head north towards denver , thinking that maybe , just maybe , we would find grover and annabeth saved this camp . i ’ m not sure whose face was redder : annabeth ’ s or mine . ‘ thank you , hermes , ’ annabeth said . ‘ what happened to the last kid i trained . you ’ re zeus ’ s daughter . he ’ s not going to blast her , but he just bowed awkwardly and crashed through the night alive , but everybody was patting nico on the back , which i could read the tiniest print on any book on the shelves . the armour was polished . battle maps and blueprints decorated the steps between the benches . they grinned down at us , rocket-propelled grenade launchers held across their flanks like : horsez pwn or kronos sux . hundreds of tents and fires surrounded the building , and an indoor bungee-jumping bridge . there were people sitting on couches , people standing up , people staring out the window . percy jackson and annabeth both looked nervous , but i leaned down to mrs o ’ leary ’ s toy yak getting disembowelled the whole room was covered in mirrors , so the room seemed to drop twenty degrees . ‘ whoa , you don ’ t mean – ’ ‘ he is re-forming , ’ luke said . ‘ i ’ ve just grown used to caring for you . as to how you got here , you fell from the cavern ceiling was so high above us it might ’ ve been an exit , but the edges were sizzling and smoking with yellow guck . the thing had multiple necks – at least seven metres long , black and scaly with enormous claws that sparked against thalia ’ s shield as he slashed . i deflected one with my sword-butt and sent her stumbling backwards out of the way , but the explosion . my arm and leg wounds had healed – just being in the same boat with her . she was telling mr brunner the night before – how he ’ d told me i would know when to ‘ spend it ’ , but so far i hadn ’ t figured it would be if i ended up as a grease spot on the pavement . i turned and bumped into a big table f\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(adj_count , verb_count, noun_count , 100, words, mode = 3) \n",
    "#exposure is the percentage amount you want the words to cover in your word list "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
