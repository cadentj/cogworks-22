{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7693ebd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOption 1:\\n\\nfrom random import choice \\n\\nnouns = (\"puppy\", \"car\", \"rabbit\", \"girl\", \"monkey\")\\nverbs = (\"runs\", \"hits\", \"jumps\", \"drives\", \"barfs\") \\nadv = (\"crazily.\", \"dutifully.\", \"foolishly.\", \"merrily.\", \"occasionally.\")\\nadj = (\"adorable\", \"clueless\", \"dirty\", \"odd\", \"stupid\")\\n\\n# asking user as to how many sentences he would like to generate \\nfor _ in range (int (input (\"Enter integer value :\"))): \\n    print(list(map(choice, [nouns, verbs, adv, adj])))\\n    \\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Option 1:\n",
    "\n",
    "from random import choice \n",
    "\n",
    "nouns = (\"puppy\", \"car\", \"rabbit\", \"girl\", \"monkey\")\n",
    "verbs = (\"runs\", \"hits\", \"jumps\", \"drives\", \"barfs\") \n",
    "adv = (\"crazily.\", \"dutifully.\", \"foolishly.\", \"merrily.\", \"occasionally.\")\n",
    "adj = (\"adorable\", \"clueless\", \"dirty\", \"odd\", \"stupid\")\n",
    "\n",
    "# asking user as to how many sentences he would like to generate \n",
    "for _ in range (int (input (\"Enter integer value :\"))): \n",
    "    print(list(map(choice, [nouns, verbs, adv, adj])))\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6d14ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1690611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(pairs):\n",
    "    \"\"\"\n",
    "    \"unzips\" of groups of items into separate tuples.\n",
    "    \n",
    "    Example: pairs = [(\"a\", 1), (\"b\", 2), ...] --> ((\"a\", \"b\", ...), (1, 2, ...))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pairs : Iterable[Tuple[Any, ...]]\n",
    "        An iterable of the form ((a0, b0, c0, ...), (a1, b1, c1, ...))\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Tuples[Any, ...], ...]\n",
    "       A tuple containing the \"unzipped\" contents of `pairs`; i.e. \n",
    "       ((a0, a1, ...), (b0, b1, ...), (c0, c1), ...)\n",
    "    \"\"\"\n",
    "    return tuple(zip(*pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5dc44db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(counter):\n",
    "    \"\"\" Convert a `letter -> count` counter to a list \n",
    "    of (letter, frequency) pairs, sorted in descending order of \n",
    "    frequency.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    counter : collections.Counter\n",
    "        letter -> count\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Tuple[str, float]]\n",
    "       A list of tuples: (letter, frequency) pairs in order\n",
    "       of descending-frequency\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from collections import Counter\n",
    "    >>> letter_count = Counter({\"a\": 1, \"b\": 3})\n",
    "    >>> letter_count\n",
    "    Counter({'a': 1, 'b': 3})\n",
    "\n",
    "    >>> normalize(letter_count)\n",
    "    [('b', 0.75), ('a', 0.25)]\n",
    "    \"\"\"\n",
    "\n",
    "    total = sum(counter.values())\n",
    "    return [(char, cnt/total) for char, cnt in counter.most_common()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de7a8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "def train_lm(text, n):\n",
    "    \"\"\" Train character-based n-gram language model.\n",
    "        \n",
    "    This will learn: given a sequence of n-1 characters, what the probability\n",
    "    distribution is for the n-th character in the sequence.\n",
    "\n",
    "    For example if we train on the text:\n",
    "        text = \"cacao\"\n",
    "\n",
    "    Using a n-gram size of n=3, then the following dict would be returned.\n",
    "    See that we *normalize* each of the char_count_tuples for a given history\n",
    "\n",
    "        {'ac': [('a', 1.0)],\n",
    "         'ca': [('c', 0.5), ('o', 0.5)],\n",
    "         '~c': [('a', 1.0)],\n",
    "         '~~': [('c', 1.0)]}\n",
    "\n",
    "    Tildas (\"~\") are used for padding the history when necessary, so that it's \n",
    "    possible to estimate the probability of a seeing a character when there \n",
    "    aren't (n - 1) previous characters of history available.\n",
    "\n",
    "    So, according to this text we trained on, if you see the sequence 'ac',\n",
    "    our model predicts that the next character should be 'a' 100% of the time.\n",
    "\n",
    "    For generating the padding, recall that Python allows you to generate \n",
    "    repeated sequences easily: \n",
    "       `\"p\" * 4` returns `\"pppp\"`\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    text: str \n",
    "        A string (doesn't need to be lowercased).\n",
    "        \n",
    "    n: int\n",
    "        The length of n-gram to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[Tuple[str, float]]]\n",
    "        \n",
    "        {n-1 history -> [(letter, normalized count), ...]}\n",
    "        \n",
    "        A dictionary that maps histories (strings of length (n-1)) to lists of (char, prob) \n",
    "        pairs, where prob is the probability (i.e frequency) of char appearing after \n",
    "        that specific history.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> train_lm(\"cacao\", 3)\n",
    "    {'ac': [('a', 1.0)],\n",
    "     'ca': [('c', 0.5), ('o', 0.5)],\n",
    "     '~c': [('a', 1.0)],\n",
    "     '~~': [('c', 1.0)]}\n",
    "    \"\"\"\n",
    "\n",
    "    raw_lm = defaultdict(Counter) # history -> {char -> count}\n",
    "    history = \"~\" * (n - 1)  # length n - 1 history\n",
    "    \n",
    "    # count number of times characters appear following different histories\n",
    "    #\n",
    "    # for char in text ...\n",
    "    #    1. Increment language model's count, given current history and character\n",
    "    #    2. Update history\n",
    "\n",
    "    for char in text:\n",
    "        raw_lm[history][char] += 1\n",
    "        # slide history window to the right by one character\n",
    "        history = history[1:] + char\n",
    "\n",
    "    \n",
    "    # create the finalized language model â€“ a dictionary with: history -> [(char, freq), ...]\n",
    "    lm = {history : normalize(counter) for history, counter in raw_lm.items()} \n",
    "    \n",
    "    return lm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a58ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_letter(lm, history):\n",
    "    \"\"\" Randomly picks letter according to probability distribution associated with \n",
    "    the specified history, as stored in your language model.\n",
    "\n",
    "    Note: returns dummy character \"~\" if history not found in model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lm: Dict[str, List[Tuple[str, float]]] \n",
    "        The n-gram language model. \n",
    "        I.e. the dictionary: history -> [(char, freq), ...]\n",
    "\n",
    "    history: str\n",
    "        A string of length (n-1) to use as context/history for generating \n",
    "        the next character.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The predicted character. '~' if history is not in language model.\n",
    "    \"\"\"\n",
    "\n",
    "    if not history in lm:\n",
    "        return \"~\"\n",
    "    letters, probs = unzip(lm[history])\n",
    "    i = np.random.choice(letters, p=probs)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10e8d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, n, nletters=100):\n",
    "    \"\"\" Randomly generates `nletters` of text by drawing from \n",
    "    the probability distributions stored in a n-gram language model \n",
    "    `lm`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lm: Dict[str, List[Tuple[str, float]]]\n",
    "        The n-gram language model. \n",
    "        I.e. the dictionary: history -> [(char, freq), ...]\n",
    "    \n",
    "    n: int\n",
    "        Order of n-gram model.\n",
    "    \n",
    "    nletters: int\n",
    "        Number of letters to randomly generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Model-generated text. Should contain `nletters` number of\n",
    "        generated characters. The pre-pended ~'s are not to be included. \n",
    "    \"\"\"\n",
    "    # <COGINST>\n",
    "    history = \"~\" * (n - 1)\n",
    "    text = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history)\n",
    "        text.append(c)\n",
    "        history = history[1:] + c\n",
    "    return \"\".join(text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf54030",
   "metadata": {},
   "source": [
    "## 2. Generating \"The Percy Jackson Series\"\n",
    "\n",
    "The next cell loads in pjallbooks.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13b571c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cogworks_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0d/dps258cx4dj0xbr633s_gv2c0000gn/T/ipykernel_20524/3219305964.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcogworks_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_data_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath_to_pj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/manyadua/Downloads/ryan-sus/capstone/MadLib/pjallbooks.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#get_data_path(\"pjolympians.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cogworks_data'"
     ]
    }
   ],
   "source": [
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "path_to_pj = \"/Users/manyadua/Downloads/ryan-sus/capstone/MadLib/pjallbooks.txt\"\n",
    "\n",
    "#get_data_path(\"pjolympians.txt\")\n",
    "\n",
    "with open(path_to_pj, \"rb\") as f:\n",
    "    pj = f.read().decode()  \n",
    "    pj = pj.lower()  \n",
    "    pj.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6705cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "percy jackson, it said. yes, the exchange went well, i see.\n",
      "i was back in the styx, or anticipation of what i was about to end\n",
      "up embedded in fifty metres to the river and the hunter\n",
      "were trying to kill each other in\n",
      "months, but between the woods and would not have come home.â€™\n",
      "the vision shut off.\n",
      "my knees buckled, and i wouldâ€™ve been terrified. the strange things she would say good morning to\n",
      "them, ask how it was going to start frothing\n",
      "at the mouth or shooting spines, but he just bowed awkwardly. â€˜i â€“ i wonâ€™t answer questions with this\n",
      "hellhound sniffing my tail!â€™\n",
      "nico looked uneasy. â€˜you are all true heroes. and as\n",
      "soon as we get percy fixed up, you must return to half-blood hill, iâ€™ll understand why everyone was eating, chiron made a\n",
      "surprise announcement: the chariot races!â€™\n",
      "murmuring broke out at all the tables â€“ excitement, fear, disbelief.\n",
      "â€˜now i know,â€™ tantalus chided. â€˜the monster may be able to do some menial\n",
      "chores. any suggestions as to where such a beast should be kennel\n"
     ]
    }
   ],
   "source": [
    "lmpj1 = train_lm(pj, 15)\n",
    "new_text = generate_text(lmpj1, 15,1000)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbda5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text =\" It was the first day of the rest of her life. This wasn't the day she was actually born, but she knew that nothing would be the same from this day forward. Although this was a bit scary to her, it was also extremely freeing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc94e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/manyadua/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7dadbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a38e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('first', 'JJ'), ('day', 'NN'), ('of', 'IN'), ('the', 'DT'), ('rest', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('life', 'NN'), ('.', '.'), ('This', 'DT'), ('was', 'VBD'), (\"n't\", 'RB'), ('the', 'DT'), ('day', 'NN'), ('she', 'PRP'), ('was', 'VBD'), ('actually', 'RB'), ('born', 'VBN'), (',', ','), ('but', 'CC'), ('she', 'PRP'), ('knew', 'VBD'), ('that', 'IN'), ('nothing', 'NN'), ('would', 'MD'), ('be', 'VB'), ('the', 'DT'), ('same', 'JJ'), ('from', 'IN'), ('this', 'DT'), ('day', 'NN'), ('forward', 'RB'), ('.', '.'), ('Although', 'IN'), ('this', 'DT'), ('was', 'VBD'), ('a', 'DT'), ('bit', 'NN'), ('scary', 'JJ'), ('to', 'TO'), ('her', 'PRP$'), (',', ','), ('it', 'PRP'), ('was', 'VBD'), ('also', 'RB'), ('extremely', 'RB'), ('freeing', 'VBG'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tok_text = word_tokenize(new_text)\n",
    "gram = nltk.pos_tag(tok_text)\n",
    "print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f098407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter noun 1: apple\n",
      "Please enter noun 2: orange\n",
      "Please enter noun 3: grapes\n",
      "Please enter noun 4: banana\n",
      "Please enter noun 5: peaches\n"
     ]
    }
   ],
   "source": [
    "list_words = []\n",
    "for i in range(1,6):\n",
    "    list_words.append(input(\"Please enter noun \" + str(i) + \": \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b04ad507",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0d/dps258cx4dj0xbr633s_gv2c0000gn/T/ipykernel_20524/3474202436.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NN\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mgram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "## [mnay, jakjdfl, ajdfkj]\n",
    "\n",
    "counter = 0 \n",
    "for i in range(len(gram)):\n",
    "    if gram[i][1] == \"NN\" and counter < 5: \n",
    "        gram[i][0] = list_words[counter]\n",
    "        counter += 1\n",
    "print(gram)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45b21dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'first', 'day', 'of', 'the', 'rest', 'of', 'her', 'life', '.', 'This', 'was', \"n't\", 'the', 'day', 'she', 'was', 'actually', 'born', ',', 'but', 'she', 'knew', 'that', 'nothing', 'would', 'be', 'the', 'same', 'from', 'this', 'day', 'forward', '.', 'Although', 'this', 'was', 'a', 'bit', 'scary', 'to', 'her', ',', 'it', 'was', 'also', 'extremely', 'freeing', '.']\n",
      "['PRP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', '.', 'DT', 'VBD', 'RB', 'DT', 'NN', 'PRP', 'VBD', 'RB', 'VBN', ',', 'CC', 'PRP', 'VBD', 'IN', 'NN', 'MD', 'VB', 'DT', 'JJ', 'IN', 'DT', 'NN', 'RB', '.', 'IN', 'DT', 'VBD', 'DT', 'NN', 'JJ', 'TO', 'PRP$', ',', 'PRP', 'VBD', 'RB', 'RB', 'VBG', '.']\n",
      "It was the first apple of the orange of her grapes . This was n't the banana she was actually born , but she knew that peaches would be the same from this day forward . Although this was a bit scary to her , it was also extremely freeing .\n"
     ]
    }
   ],
   "source": [
    "## tokenize two arrays \n",
    "## it \n",
    "## NN \n",
    "\n",
    "words = [] \n",
    "parts_of_speech = []\n",
    "\n",
    "for i in gram: \n",
    "    words.append(i[0])\n",
    "    parts_of_speech.append(i[1])\n",
    "\n",
    "print(words)\n",
    "print(parts_of_speech)\n",
    "\n",
    "\n",
    "print(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb7f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode 1 - just nouns \n",
    "# adjective, nouns, verbs \n",
    "# crazy mode - cover more of the text with adjs, nouns, verbs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d11fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ff198d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "c = Counter(parts_of_speech)\n",
    "\n",
    "adj_count = c[\"NN\"]\n",
    "verb_count = c[\"VB\"] + c[\"VBR\"]\n",
    "noun_count = c[\"JJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f42f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text(mode = 1, adj_count, verb_count, noun_count, exposure) \n",
    "#exposure is the percentage amount you want the words to cover in your word list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_words(code, w_max, wds)\n",
    "    counter = 0 \n",
    "    for j in range(len(parts_of_speech)): \n",
    "        if parts_of_speech[j] == code and counter < w_max :\n",
    "            wds[j] = list_words[counter]\n",
    "                counter += 1\n",
    "    return wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(mode = None, adj_c, verb_c, noun_c, exp): #pass in the count of adjs, nouns, verbs, amount of exp and mode \n",
    "    \n",
    "    final_text = \"\" #stores the updated text \n",
    "    noun_max = (exp/100) * noun_c \n",
    "    adj_max = (exp/100) * adj_c\n",
    "    verb_max = (exp/100) * verb_c\n",
    "    # it is \n",
    "    if mode == 1: #only use verb_count \n",
    "        words = switch_words(\"NN\", noun_max, words)\n",
    "        final_text = ' '.join(words)\n",
    "    \n",
    "    elif mode == 2: \n",
    "        #nouns are already updated, update verbs + adjs\n",
    "        words = switch_words(\"NN\", noun_max, words)\n",
    "        words = switch_words(\"VB\", noun_max, words)\n",
    "        words = switch_words(\"JJ\", noun_max, words)\n",
    "\n",
    "        final_text = ' '.join(words)\n",
    "        \n",
    "    elif mode == 3: #run on our N-gram model \n",
    "        \n",
    "        \n",
    "    return final_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a27853",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0d/dps258cx4dj0xbr633s_gv2c0000gn/T/ipykernel_20524/1173709148.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgram\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# (word, part of speech)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NN\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "#counter = 0 \n",
    "for value in gram: # (word, part of speech)\n",
    "    if value[1] == \"NN\" and counter < 5: \n",
    "        value[0] = list_words[counter]\n",
    "        counter += 1\n",
    "        \n",
    "print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6266c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Convert(tup, di):\n",
    "#     di = dict(tup)\n",
    "#     return di\n",
    "# # Driver Code    \n",
    "# gram_dict = {}\n",
    "# Convert(gram, gram_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa356278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(list)\n",
    "for key, value in gram:\n",
    "    d[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cac1475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'It': ['PRP'], 'was': ['VBD', 'VBD', 'VBD', 'VBD', 'VBD'], 'the': ['DT', 'DT', 'DT', 'DT'], 'first': ['JJ'], 'day': ['NN', 'NN', 'NN'], 'of': ['IN', 'IN'], 'rest': ['NN'], 'her': ['PRP$', 'PRP$'], 'life': ['NN'], '.': ['.', '.', '.'], 'This': ['DT'], \"n't\": ['RB'], 'she': ['PRP', 'PRP'], 'actually': ['RB'], 'born': ['VBN'], ',': [',', ','], 'but': ['CC'], 'knew': ['VBD'], 'that': ['IN'], 'nothing': ['NN'], 'would': ['MD'], 'be': ['VB'], 'same': ['JJ'], 'from': ['IN'], 'this': ['DT', 'DT'], 'forward': ['RB'], 'Although': ['IN'], 'a': ['DT'], 'bit': ['NN'], 'scary': ['JJ'], 'to': ['TO'], 'it': ['PRP'], 'also': ['RB'], 'extremely': ['RB'], 'freeing': ['VBG']})\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "840a10cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'was',\n",
       " 'the',\n",
       " 'first',\n",
       " 'day',\n",
       " 'of',\n",
       " 'rest',\n",
       " 'her',\n",
       " 'life',\n",
       " '.',\n",
       " 'This',\n",
       " \"n't\",\n",
       " 'she',\n",
       " 'actually',\n",
       " 'born',\n",
       " ',',\n",
       " 'but',\n",
       " 'knew',\n",
       " 'that',\n",
       " 'nothing',\n",
       " 'would',\n",
       " 'be',\n",
       " 'same',\n",
       " 'from',\n",
       " 'this',\n",
       " 'forward',\n",
       " 'Although',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'scary',\n",
       " 'to',\n",
       " 'it',\n",
       " 'also',\n",
       " 'extremely',\n",
       " 'freeing']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = []\n",
    "for i in range(1,6):\n",
    "    list_words.append(input(\"Please enter noun \" + str(i) + \": \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c5167c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grapes\n",
      "orange\n",
      "peach\n",
      "banana\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list_words)):\n",
    "    print(list_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf481d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0d/dps258cx4dj0xbr633s_gv2c0000gn/T/ipykernel_20524/1342738188.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# VBD: the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#NN : apple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mg_swap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# swap the values and the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "# PRP: It\n",
    "# VBD: the\n",
    "#NN : apple\n",
    "print(g)\n",
    "g_swap = {v: k for k, v in g.items()} # swap the values and the keys \n",
    "print(\"\")\n",
    "print(g_swap)\n",
    "\n",
    "for key, value in list(g_swap.items()):\n",
    "#     print(value)\n",
    "#     print(key)\n",
    "    if key == 'NN' and counter < 5 :\n",
    "        g_swap[key] = list_words[counter]\n",
    "        #print(list_words[counter])\n",
    "        #print(g[key])\n",
    "        counter += 1\n",
    "\n",
    "new_line = list(g_swap.values()) \n",
    "\n",
    "print(new_line)\n",
    "\n",
    "## NN, Grapes \n",
    "## IN , apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"i_of_nouns = []\n",
    "\n",
    "for key, value in list(g.items()):\n",
    "    if value == 'NN':\n",
    "            del g[key]\n",
    "   \n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2885a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "196e7c86",
   "metadata": {},
   "source": [
    "## Working on making the actual game\n",
    "\n",
    "1. remove random nouns using spacy\n",
    "2. find the pos of the random words\n",
    "3. either leave word blank for user to fill in\n",
    "4. or have computer generate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9158532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f2ce74",
   "metadata": {},
   "source": [
    "## 3. Generating \"Dr. Suess' Work\"\n",
    "\n",
    "The next cell loads in drsuess.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "904a58b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 8, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_5/5tppy8rs7bv1n7qz5flpdysh0000gn/T/ipykernel_96671/2320990424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath_to_seuss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/drseuss.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mseuss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_seuss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#with open(path_to_suess, \"rb\") as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 8, saw 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_to_seuss = \"/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/drseuss.txt\"\n",
    "\n",
    "seuss = pd.read_csv(path_to_seuss, encoding= 'unicode_escape')\n",
    "\n",
    "#with open(path_to_suess, \"rb\") as f:\n",
    "seuss = seuss.read().decode()  \n",
    "seuss = seuss.lower()\n",
    "seuss.split()\n",
    "    \n",
    "\n",
    "\n",
    "print(str(len(suess)) + \" character(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41575302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
