{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Option 1:\n",
    "\n",
    "from random import choice \n",
    "\n",
    "nouns = (\"puppy\", \"car\", \"rabbit\", \"girl\", \"monkey\")\n",
    "verbs = (\"runs\", \"hits\", \"jumps\", \"drives\", \"barfs\") \n",
    "adv = (\"crazily.\", \"dutifully.\", \"foolishly.\", \"merrily.\", \"occasionally.\")\n",
    "adj = (\"adorable\", \"clueless\", \"dirty\", \"odd\", \"stupid\")\n",
    "\n",
    "# asking user as to how many sentences he would like to generate \n",
    "for _ in range (int (input (\"Enter integer value :\"))): \n",
    "    print(list(map(choice, [nouns, verbs, adv, adj])))\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d14ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc44db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(counter):\n",
    "    \"\"\" Convert a `letter -> count` counter to a list \n",
    "    of (letter, frequency) pairs, sorted in descending order of \n",
    "    frequency.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    counter : collections.Counter\n",
    "        letter -> count\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Tuple[str, float]]\n",
    "       A list of tuples: (letter, frequency) pairs in order\n",
    "       of descending-frequency\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from collections import Counter\n",
    "    >>> letter_count = Counter({\"a\": 1, \"b\": 3})\n",
    "    >>> letter_count\n",
    "    Counter({'a': 1, 'b': 3})\n",
    "\n",
    "    >>> normalize(letter_count)\n",
    "    [('b', 0.75), ('a', 0.25)]\n",
    "    \"\"\"\n",
    "\n",
    "    total = sum(counter.values())\n",
    "    return [(char, cnt/total) for char, cnt in counter.most_common()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de7a8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train_lm(text, n):\n",
    "    \"\"\" Train character-based n-gram language model.\n",
    "        \n",
    "    This will learn: given a sequence of n-1 characters, what the probability\n",
    "    distribution is for the n-th character in the sequence.\n",
    "\n",
    "    For example if we train on the text:\n",
    "        text = \"cacao\"\n",
    "\n",
    "    Using a n-gram size of n=3, then the following dict would be returned.\n",
    "    See that we *normalize* each of the char_count_tuples for a given history\n",
    "\n",
    "        {'ac': [('a', 1.0)],\n",
    "         'ca': [('c', 0.5), ('o', 0.5)],\n",
    "         '~c': [('a', 1.0)],\n",
    "         '~~': [('c', 1.0)]}\n",
    "\n",
    "    Tildas (\"~\") are used for padding the history when necessary, so that it's \n",
    "    possible to estimate the probability of a seeing a character when there \n",
    "    aren't (n - 1) previous characters of history available.\n",
    "\n",
    "    So, according to this text we trained on, if you see the sequence 'ac',\n",
    "    our model predicts that the next character should be 'a' 100% of the time.\n",
    "\n",
    "    For generating the padding, recall that Python allows you to generate \n",
    "    repeated sequences easily: \n",
    "       `\"p\" * 4` returns `\"pppp\"`\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    text: str \n",
    "        A string (doesn't need to be lowercased).\n",
    "        \n",
    "    n: int\n",
    "        The length of n-gram to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[Tuple[str, float]]]\n",
    "        \n",
    "        {n-1 history -> [(letter, normalized count), ...]}\n",
    "        \n",
    "        A dictionary that maps histories (strings of length (n-1)) to lists of (char, prob) \n",
    "        pairs, where prob is the probability (i.e frequency) of char appearing after \n",
    "        that specific history.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> train_lm(\"cacao\", 3)\n",
    "    {'ac': [('a', 1.0)],\n",
    "     'ca': [('c', 0.5), ('o', 0.5)],\n",
    "     '~c': [('a', 1.0)],\n",
    "     '~~': [('c', 1.0)]}\n",
    "    \"\"\"\n",
    "\n",
    "    raw_lm = defaultdict(Counter) # history -> {char -> count}\n",
    "    history = \"~\" * (n - 1)  # length n - 1 history\n",
    "    \n",
    "    # count number of times characters appear following different histories\n",
    "    #\n",
    "    # for char in text ...\n",
    "    #    1. Increment language model's count, given current history and character\n",
    "    #    2. Update history\n",
    "\n",
    "    for char in text:\n",
    "        raw_lm[history][char] += 1\n",
    "        # slide history window to the right by one character\n",
    "        history = history[1:] + char\n",
    "\n",
    "    \n",
    "    # create the finalized language model â€“ a dictionary with: history -> [(char, freq), ...]\n",
    "    lm = {history : normalize(counter) for history, counter in raw_lm.items()} \n",
    "    \n",
    "    return lm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a58ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_letter(lm, history):\n",
    "    \"\"\" Randomly picks letter according to probability distribution associated with \n",
    "    the specified history, as stored in your language model.\n",
    "\n",
    "    Note: returns dummy character \"~\" if history not found in model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lm: Dict[str, List[Tuple[str, float]]] \n",
    "        The n-gram language model. \n",
    "        I.e. the dictionary: history -> [(char, freq), ...]\n",
    "\n",
    "    history: str\n",
    "        A string of length (n-1) to use as context/history for generating \n",
    "        the next character.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The predicted character. '~' if history is not in language model.\n",
    "    \"\"\"\n",
    "\n",
    "    if not history in lm:\n",
    "        return \"~\"\n",
    "    letters, probs = unzip(lm[history])\n",
    "    i = np.random.choice(letters, p=probs)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e8d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, n, nletters=100):\n",
    "    \"\"\" Randomly generates `nletters` of text by drawing from \n",
    "    the probability distributions stored in a n-gram language model \n",
    "    `lm`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lm: Dict[str, List[Tuple[str, float]]]\n",
    "        The n-gram language model. \n",
    "        I.e. the dictionary: history -> [(char, freq), ...]\n",
    "    \n",
    "    n: int\n",
    "        Order of n-gram model.\n",
    "    \n",
    "    nletters: int\n",
    "        Number of letters to randomly generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Model-generated text. Should contain `nletters` number of\n",
    "        generated characters. The pre-pended ~'s are not to be included. \n",
    "    \"\"\"\n",
    "    # <COGINST>\n",
    "    history = \"~\" * (n - 1)\n",
    "    text = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history)\n",
    "        text.append(c)\n",
    "        history = history[1:] + c\n",
    "    return \"\".join(text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8a4d1",
   "metadata": {},
   "source": [
    "## 1. Generating \"Shakespeare\"\n",
    "\n",
    "The next cell loads in Andrej Karpathy's shakespeare_input.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d8f14ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m path_to_shakespeare \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_path\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshakespeare_input.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_to_shakespeare, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     shakespeare \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_data_path' is not defined"
     ]
    }
   ],
   "source": [
    "path_to_shakespeare = get_data_path(\"shakespeare_input.txt\")\n",
    "\n",
    "with open(path_to_shakespeare, \"r\") as f:\n",
    "    shakespeare = f.read()\n",
    "print(str(len(shakespeare)) + \" character(s)\")\n",
    "chars = set(shakespeare)\n",
    "print(f\"'~' is a good pad character: {'~' not in chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b13df95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shakespeare' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m lm11 \u001b[38;5;241m=\u001b[39m train_lm(\u001b[43mshakespeare\u001b[49m, \u001b[38;5;241m11\u001b[39m)\n\u001b[1;32m      3\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(t1 \u001b[38;5;241m-\u001b[39m t0) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shakespeare' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "lm11 = train_lm(shakespeare, 11)\n",
    "t1 = time.time()\n",
    "print(\"elapsed = \" + str(t1 - t0) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7496a14d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lm11' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(generate_text(\u001b[43mlm11\u001b[49m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lm11' is not defined"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm11, 11, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf54030",
   "metadata": {},
   "source": [
    "## 2. Generating \"The Percy Jackson Series\"\n",
    "\n",
    "The next cell loads in pjallbooks.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13b571c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/pjallbooks.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m path_to_pj \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/pjallbooks.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#get_data_path(\"pjolympians.txt\")\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_to_pj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     pj \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode()  \n\u001b[1;32m      9\u001b[0m     pj \u001b[38;5;241m=\u001b[39m pj\u001b[38;5;241m.\u001b[39mlower()  \n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/pjallbooks.txt'"
     ]
    }
   ],
   "source": [
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "path_to_pj = \"/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/pjallbooks.txt\"\n",
    "\n",
    "#get_data_path(\"pjolympians.txt\")\n",
    "\n",
    "with open(path_to_pj, \"rb\") as f:\n",
    "    pj = f.read().decode()  \n",
    "    pj = pj.lower()  \n",
    "    pj.split()\n",
    "\n",
    "lmpj1 = train_lm(pj, 50)\n",
    "print(generate_text(lmpj1, 50, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e7c86",
   "metadata": {},
   "source": [
    "## Working on making the actual game\n",
    "\n",
    "1. remove random nouns using spacy\n",
    "2. find the pos of the random words\n",
    "3. either leave word blank for user to fill in\n",
    "4. or have computer generate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9158532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f2ce74",
   "metadata": {},
   "source": [
    "## 3. Generating \"Dr. Suess' Work\"\n",
    "\n",
    "The next cell loads in drsuess.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "904a58b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 8, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_5/5tppy8rs7bv1n7qz5flpdysh0000gn/T/ipykernel_96671/2320990424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath_to_seuss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/drseuss.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mseuss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_seuss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#with open(path_to_suess, \"rb\") as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 8, saw 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_to_seuss = \"/Users/mohan/Desktop/cogworks/bwsi/ryan-sus/capstone/MadLib/drseuss.txt\"\n",
    "\n",
    "seuss = pd.read_csv(path_to_seuss, encoding= 'unicode_escape')\n",
    "\n",
    "#with open(path_to_suess, \"rb\") as f:\n",
    "seuss = seuss.read().decode()  \n",
    "seuss = seuss.lower()\n",
    "seuss.split()\n",
    "    \n",
    "\n",
    "\n",
    "print(str(len(suess)) + \" character(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41575302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
